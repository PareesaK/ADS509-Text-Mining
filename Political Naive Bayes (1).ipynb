{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parisa Kamizi Assignment 4.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes on Political Text\n",
    "\n",
    "In this notebook we use Naive Bayes to explore and classify political data. See the `README.md` for full details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatGPT was utilized on the assignment as a learning tool. \n",
    "\n",
    "import sqlite3\n",
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# Feel free to include your text patterns functions\n",
    "#from text_functions_solutions import clean_tokenize, get_patterns\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "import string\n",
    "from sklearn.metrics import classification_report\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x7fbb1806bb90>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convention_db = sqlite3.connect(\"2020_Conventions.db\")\n",
    "convention_cur = convention_db.cursor()\n",
    "convention_cur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables in the database: [('conventions',)]\n"
     ]
    }
   ],
   "source": [
    "convention_db = sqlite3.connect('/Users/parisakamizi/Downloads/2020_Conventions.db')  \n",
    "convention_cur = convention_db.cursor()\n",
    "\n",
    "tables = convention_cur.execute(\"SELECT name FROM sqlite_master WHERE type='table';\").fetchall()\n",
    "print(\"Tables in the database:\", tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Exploratory Naive Bayes\n",
    "\n",
    "We'll first build a NB model on the convention data itself, as a way to understand what words distinguish between the two parties. This is analogous to what we did in the \"Comparing Groups\" class work. First, pull in the text \n",
    "for each party and prepare it for use in Naive Bayes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['I’m here by calling the full session of the 48th Quadrennial National Convention of the Democratic Party to order. Welcome all to our final session of this historic and memorable convention. We’ve called the 48th Quadrennial Democratic National Convention to order.', 'Democratic'], ['Every four years, we come together to reaffirm our democracy. This year, we’ve come to save it.', 'Democratic'], ['We fight for a more perfect union because we are fighting for the soul of this country and for our lives. And right now that fight is real.', 'Democratic'], ['We must come together to defeat Donald Trump, and elect Joe Biden and Kamala Harris as our next President and Vice President.', 'Democratic'], ['Donald Trump is the wrong President for our country. He has had more than enough time to prove that he can do the job, but he is clearly in over his head. He simply cannot be who we need him to be for us. It is what it is.', 'Democratic']]\n"
     ]
    }
   ],
   "source": [
    "# Resources can be found from --> https://docs.python.org/3/library/sqlite3.html\n",
    "# and, https://www.nltk.org/\n",
    " \n",
    "convention_data = []\n",
    "\n",
    "# fill this list up with items that are themselves lists. The \n",
    "# first element in the sublist should be the cleaned and tokenized\n",
    "# text in a single string. The second element should be the party. \n",
    "\n",
    "query_results = convention_cur.execute(\n",
    "                            '''\n",
    "                            SELECT text, party\n",
    "                            FROM conventions\n",
    "                            WHERE speaker != \"Unknown\"\n",
    "                            ''')\n",
    "\n",
    "for row in query_results:\n",
    "    if row[0]:  \n",
    "       convention_data.append([row[0].strip(), row[1]])  \n",
    "\n",
    "print(convention_data[:5])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some random entries and see if they look right. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['(singing) Mejente, let’s stand by each other. Don’t forget to vote this November. Together we can make a chance. [Spanish 01:07:16] Let’s go.',\n",
       "  'Democratic'],\n",
       " ['In closing, I’d like to speak directly to my father. I miss working alongside you every single day, but I’m damn proud to be on the front lines of this fight. I’m proud of what you’re doing for this country. I’m proud to show my children what their grandfather is fighting for. I’m proud to watch you give them hell. Never stop. Continue to be unapologetic. Keep fighting for what is right. You are making America strong again. You are making America safe again. You are making America proud again. And yes, together with our forgotten men and women who are finally forgotten no more, you are making America great again. Dad, let’s make Uncle Robert very proud this week. Let’s go get another four years. I love you very much. God bless you. And God bless the United States of America.',\n",
       "  'Republican'],\n",
       " ['Ladies and gentlemen, let’s re-elect President Trump. Let’s figure out what the problems are and continue to find solutions to those problems. Then, let’s get to work. I’d like to close with this. While it’s critically important to re-elect President Trump, this pandemic has also taught us to be very, very careful who you select as your next governor, senator, congressperson and mayor. It is so important to vote. Don’t think that your vote doesn’t matter, because to be honest with you, it has never mattered more than it does right now. Thank you and have a great evening, America.',\n",
       "  'Republican'],\n",
       " ['I don’t think we can deal with the type of person we have in the White House any longer.',\n",
       "  'Democratic'],\n",
       " ['She is courageous. She’s taking on a tough topic.', 'Republican'],\n",
       " ['So we need numbers overwhelming so Trump can’t sneak or steal his way to victory. Text Vote 30330 to get started. A hundred years ago yesterday the 19th amendment to the constitution was ratified. It took seven decades of suffragists marching, picketing and going to jail to push us closer to a more perfect union. 55 years ago John Lewis marched and bled in Selma because that work was unfinished. Tonight I’m thinking of the girls and boys who see themselves in America’s future because of Kamala Harris. A black woman, the daughter of Jamaican and Indian immigrants and our nominee for vice president. This is our country’s story, breaking down barriers and expanding the circle of possibility.',\n",
       "  'Democratic'],\n",
       " ['Beau isn’t with us any longer.', 'Democratic'],\n",
       " ['We will run. I ran for president because I think it’s urgent that we heal the divisions in this nation.',\n",
       "  'Democratic'],\n",
       " ['Tennessee.', 'Democratic'],\n",
       " ['Questions about money he made from foreign business dealings while his father was vice president.',\n",
       "  'Republican']]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choices(convention_data,k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If that looks good, we now need to make our function to turn these into features. In my solution, I wanted to keep the number of features reasonable, so I only used words that occur at least `word_cutoff` times. Here's the code to test that if you want it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With a word cutoff of 5, we have 2845 as features in the model.\n"
     ]
    }
   ],
   "source": [
    "word_cutoff = 5\n",
    "\n",
    "tokens = [w for t, p in convention_data for w in t.split()]\n",
    "\n",
    "word_dist = nltk.FreqDist(tokens)\n",
    "\n",
    "feature_words = set()\n",
    "\n",
    "for word, count in word_dist.items() :\n",
    "    if count > word_cutoff :\n",
    "        feature_words.add(word)\n",
    "        \n",
    "print(f\"With a word cutoff of {word_cutoff}, we have {len(feature_words)} as features in the model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resources can be found from here --> https://docs.python.org/3/library/stdtypes.html#string-methods\n",
    "# https://docs.python.org/3/library/stdtypes.html#mapping-types-dict\n",
    "# https://docs.python.org/3/library/stdtypes.html#str.lower\n",
    "# https://docs.python.org/3/library/stdtypes.html#str.split\n",
    "# and, https://docs.python.org/3/library/stdtypes.html#str.translate\n",
    "\n",
    "def conv_features(text,fw) :\n",
    "    \"\"\"Given some text, this returns a dictionary holding the\n",
    "       feature words.\n",
    "       \n",
    "       Args: \n",
    "            * text: a piece of text in a continuous string. Assumes\n",
    "            text has been cleaned and case folded.\n",
    "            * fw: the *feature words* that we're considering. A word \n",
    "            in `text` must be in fw in order to be returned. This \n",
    "            prevents us from considering very rarely occurring words.\n",
    "        \n",
    "       Returns: \n",
    "            A dictionary with the words in `text` that appear in `fw`. \n",
    "            Words are only counted once. \n",
    "            If `text` were \"quick quick brown fox\" and `fw` = {'quick','fox','jumps'},\n",
    "            then this would return a dictionary of \n",
    "            {'quick' : True,\n",
    "             'fox' :    True}\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # Your code here\n",
    "    \n",
    "    # Case-folding the text and removing punctuation\n",
    "    text = text.lower().translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    \n",
    "    # Split the text into words\n",
    "    words = text.split()\n",
    "    \n",
    "    # Create a dictionary for feature words \n",
    "    ret_dict = {word: True for word in words if word in fw}\n",
    "    \n",
    "    return ret_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_words = {'donald', 'president', 'america', 'american', 'people'}\n",
    "\n",
    "assert(len(feature_words)>0)\n",
    "assert(conv_features(\"donald is the president\",feature_words)==\n",
    "       {'donald':True,'president':True})\n",
    "assert(conv_features(\"people are american in america\",feature_words)==\n",
    "                     {'america':True,'american':True,\"people\":True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll build our feature set. Out of curiosity I did a train/test split to see how accurate the classifier was, but we don't strictly need to since this analysis is exploratory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I’m here by calling the full session of the 48th Quadrennial National Convention of the Democratic Party to order. Welcome all to our final session of this historic and memorable convention. We’ve called the 48th Quadrennial Democratic National Convention to order.', 'Democratic'), ('Every four years, we come together to reaffirm our democracy. This year, we’ve come to save it.', 'Democratic'), ('We fight for a more perfect union because we are fighting for the soul of this country and for our lives. And right now that fight is real.', 'Democratic'), ('We must come together to defeat Donald Trump, and elect Joe Biden and Kamala Harris as our next President and Vice President.', 'Democratic'), ('Donald Trump is the wrong President for our country. He has had more than enough time to prove that he can do the job, but he is clearly in over his head. He simply cannot be who we need him to be for us. It is what it is.', 'Democratic')]\n"
     ]
    }
   ],
   "source": [
    "# Resources can be found from here --> https://stackoverflow.com/questions/29438265/stratified-train-test-split-in-scikit-learn\n",
    "# and https://docs.python.org/3/tutorial/datastructures.html\n",
    "\n",
    "democratic_speeches = []\n",
    "republican_speeches = []\n",
    "\n",
    "for text, party in convention_data:\n",
    "    if party == 'Democratic':\n",
    "        democratic_speeches.append(text)\n",
    "    elif party == 'Republican':\n",
    "        republican_speeches.append(text)\n",
    "balanced_data = []\n",
    "\n",
    "for text in democratic_speeches:\n",
    "    balanced_data.append((text, 'Democratic'))\n",
    "\n",
    "for text in republican_speeches:\n",
    "    balanced_data.append((text, 'Republican'))\n",
    "\n",
    "print(balanced_data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced data includes 1941 speeches.\n"
     ]
    }
   ],
   "source": [
    "# Resources can be found from here --> https://docs.python.org/3/library/collections.html#collections.Counter\n",
    "# https://stackoverflow.com/questions/49328319/random-sampling-from-a-column-several-times-in-python-pandas\n",
    "# https://www.geeksforgeeks.org/text-summarization-in-nlp/\n",
    "# and, https://stackoverflow.com/questions/1637807/modifying-list-while-iterating\n",
    "\n",
    "# Step 1: Count speeches for each party\n",
    "d_count, r_count = Counter([p for t, p in convention_data])['Democratic'], Counter([p for t, p in convention_data])['Republican']\n",
    "\n",
    "# Step 2: Calculate the median text length\n",
    "median_text = np.median([len(t.split()) for t, p in convention_data])\n",
    "\n",
    "# Step 3: Filter speeches \n",
    "new_convention_data = [\n",
    "    [text, party] for text, party in convention_data if party == \"Republican\" or len(text.split()) >= median_text\n",
    "]\n",
    "\n",
    "# Step 4: Handle short Democratic speeches\n",
    "short_texts = [text for text, party in convention_data if party == \"Democratic\" and len(text.split()) < median_text]\n",
    "cur_d_count = Counter([p for t, p in new_convention_data])['Democratic']\n",
    "\n",
    "# Merge short speeches until counts match\n",
    "while cur_d_count < r_count and short_texts:\n",
    "    cur_text = \"\"  \n",
    "    while len(cur_text.split()) < median_text and short_texts:\n",
    "        cur_text += \" \" + short_texts.pop()  \n",
    "    new_convention_data.append([cur_text.strip(), 'Democratic'])\n",
    "    cur_d_count += 1\n",
    "\n",
    "# Step 5: Randomly append remaining short texts if necessary\n",
    "if cur_d_count == r_count and short_texts:\n",
    "    for idx, (text, party) in enumerate(new_convention_data):\n",
    "        if short_texts:\n",
    "            new_convention_data[idx][0] = \" \".join([text.split(), short_texts.pop()])\n",
    "\n",
    "print(f\"Balanced data includes {len(new_convention_data)} speeches.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = [(conv_features(text, feature_words), party) for text, party in new_convention_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(20220507)\n",
    "random.shuffle(featuresets)\n",
    "\n",
    "test_size = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.552\n"
     ]
    }
   ],
   "source": [
    "test_set, train_set = featuresets[:test_size], featuresets[test_size:]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                american = True           Republ : Democr =      1.7 : 1.0\n",
      "                  donald = True           Republ : Democr =      1.4 : 1.0\n",
      "                 america = True           Republ : Democr =      1.3 : 1.0\n",
      "               president = True           Republ : Democr =      1.2 : 1.0\n",
      "                  people = True           Democr : Republ =      1.2 : 1.0\n",
      "               president = None           Democr : Republ =      1.1 : 1.0\n",
      "                american = None           Democr : Republ =      1.1 : 1.0\n",
      "                 america = None           Democr : Republ =      1.1 : 1.0\n",
      "                  donald = None           Democr : Republ =      1.0 : 1.0\n",
      "                  people = None           Republ : Democr =      1.0 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Democratic       0.75      0.15      0.25       249\n",
      "  Republican       0.53      0.95      0.68       251\n",
      "\n",
      "    accuracy                           0.55       500\n",
      "   macro avg       0.64      0.55      0.47       500\n",
      "weighted avg       0.64      0.55      0.47       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = [classifier.classify(features) for features, _ in test_set]\n",
    "true_labels = [label for _, label in test_set]\n",
    "\n",
    "print(classification_report(true_labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'Republican': 986, 'Democratic': 955})\n"
     ]
    }
   ],
   "source": [
    "# Resources cane be found here --> https://docs.python.org/3/library/collections.html#collections.Counter\n",
    "\n",
    "party_counts = Counter([p for t, p in new_convention_data])\n",
    "\n",
    "print(party_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a little prose here about what you see in the classifier. Anything odd or interesting?\n",
    "\n",
    "\n",
    "### My Observations\n",
    "\n",
    "_Your observations to come._\n",
    "\n",
    "The Naive Bayes classifier shows moderate performance with 55% accuracy, revealing key political terms tied to the Republican party, while suggesting room for improvement through better text analysis techniques.\n",
    "The model performs better for Republican speeches, correctly identifying most of them, but it struggles with precision, often misclassifying non-Republican speeches. On the other hand, it misses many Democratic speeches, as indicated by the low recall. Overall, the accuracy is low, and the model seems to be biased towards the Republican party.\n",
    "To improve the model, I believe enhancing the data cleaning and tokenization process could help by ensuring that the text is properly preprocessed, removing unnecessary noise. Additionally, fine-tuning the model with better feature extraction and experimenting with class weighting could improve recall for Democratic speeches.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Classifying Congressional Tweets\n",
    "\n",
    "In this part we apply the classifer we just built to a set of tweets by people running for congress\n",
    "in 2018. These tweets are stored in the database `congressional_data.db`. That DB is funky, so I'll\n",
    "give you the query I used to pull out the tweets. Note that this DB has some big tables and \n",
    "is unindexed, so the query takes a minute or two to run on my machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resources can be found from here --> https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\n",
    "# Moudle 2 and 3 assignment\n",
    "# and https://www.nltk.org/api/nltk.tokenize.html\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_tokenize(text):\n",
    "    if isinstance(text, bytes):  \n",
    "        text = text.decode('utf-8')  \n",
    "    tokens = word_tokenize(text.lower())  \n",
    "    cleaned_tokens = [word for word in tokens \n",
    "                      if word.isalpha() and word not in stop_words]  \n",
    "    return ' '.join(cleaned_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "cong_db = sqlite3.connect(\"congressional_data.db\")\n",
    "cong_cur = cong_db.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables in the database: [('websites',), ('candidate_data',), ('tweets',)]\n"
     ]
    }
   ],
   "source": [
    "cong_db = sqlite3.connect('/Users/parisakamizi/Downloads/congressional_data.db')  \n",
    "cong_cur = cong_db.cursor()\n",
    "\n",
    "tables = cong_db.execute(\"SELECT name FROM sqlite_master WHERE type='table';\").fetchall()\n",
    "print(\"Tables in the database:\", tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = cong_cur.execute(\n",
    "        '''\n",
    "           SELECT DISTINCT \n",
    "                  cd.candidate, \n",
    "                  cd.party,\n",
    "                  tw.tweet_text\n",
    "           FROM candidate_data cd \n",
    "           INNER JOIN tweets tw ON cd.twitter_handle = tw.handle \n",
    "               AND cd.candidate == tw.candidate \n",
    "               AND cd.district == tw.district\n",
    "           WHERE cd.party in ('Republican','Democratic') \n",
    "               AND tw.tweet_text NOT LIKE '%RT%'\n",
    "        ''')\n",
    "\n",
    "results = list(results) \n",
    "\n",
    "cong_db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now fill up tweet_data with sublists like we did on the convention speeches.\n",
    "# Note that this may take a bit of time, since we have a lot of tweets.\n",
    "# Resources can be found here --> https://www.nltk.org/api/nltk.tokenize.html\n",
    "# https://www.analyticsvidhya.com/blog/2022/01/text-cleaning-methods-in-nlp/\n",
    "# and, https://docs.python.org/3/tutorial/datastructures.html#more-on-lists\n",
    "\n",
    "tweet_data = []\n",
    "for row in results:\n",
    "    tweet_text = row[2]  \n",
    "    party = row[1]  \n",
    "\n",
    "    cleaned_tweet = clean_tokenize(tweet_text)  \n",
    "    tweet_data.append([cleaned_tweet, party])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 532,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Resources can be found from here --> https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "# and, https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html\n",
    "\n",
    "X = [tweet[0] for tweet in tweet_data]  \n",
    "y = [tweet[1] for tweet in tweet_data]  \n",
    "\n",
    "# Vectorize the tweets using CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X_vec = vectorizer.fit_transform(X)\n",
    "\n",
    "# Train a Naive Bayes classifier\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_vec, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of tweets here. Let's take a random sample and see how our classifer does. I'm guessing it won't be too great given the performance on the convention speeches..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(20201014)\n",
    "\n",
    "tweet_data_sample = random.choices(tweet_data,k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's our (cleaned) tweet: earlier today spoke house floor abt protecting health care women praised ppmarmonte work central coast https\n",
      "Actual party is Democratic and our classifier says Democratic.\n",
      "\n",
      "Here's our (cleaned) tweet: go tribe rallytogether https\n",
      "Actual party is Democratic and our classifier says Democratic.\n",
      "\n",
      "Here's our (cleaned) tweet: apparently trump thinks easy students overwhelmed crushing burden debt pay student loans trumpbudget https\n",
      "Actual party is Democratic and our classifier says Democratic.\n",
      "\n",
      "Here's our (cleaned) tweet: grateful first responders rescue personnel firefighters police volunteers working tirelessly keep people safe provide help putting lives line https\n",
      "Actual party is Republican and our classifier says Democratic.\n",
      "\n",
      "Here's our (cleaned) tweet: let make even greater kag https\n",
      "Actual party is Republican and our classifier says Republican.\n",
      "\n",
      "Here's our (cleaned) tweet: cavs tie series repbarbaralee scared roadtovictory\n",
      "Actual party is Democratic and our classifier says Democratic.\n",
      "\n",
      "Here's our (cleaned) tweet: congrats belliottsd new gig sd city hall glad continue https\n",
      "Actual party is Democratic and our classifier says Democratic.\n",
      "\n",
      "Here's our (cleaned) tweet: really close raised toward match right whoot majors room help us get https https\n",
      "Actual party is Democratic and our classifier says Democratic.\n",
      "\n",
      "Here's our (cleaned) tweet: today comment period potus plan expand offshore drilling opened public days march share oppose proposed program directly trump administration comments made email mail https\n",
      "Actual party is Democratic and our classifier says Democratic.\n",
      "\n",
      "Here's our (cleaned) tweet: celebrated icseastla years eastside commitment amp saluted community leaders last night awards dinner https\n",
      "Actual party is Democratic and our classifier says Democratic.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for tweet, party in tweet_data_sample:\n",
    "    tweet_vec = vectorizer.transform([tweet])  \n",
    "    estimated_party = clf.predict(tweet_vec)[0]  \n",
    "    print(f\"Here's our (cleaned) tweet: {tweet}\")\n",
    "    print(f\"Actual party is {party} and our classifier says {estimated_party}.\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've looked at it some, let's score a bunch and see how we're doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resources can be found from here --> https://stackoverflow.com/questions/29438265/stratified-train-test-split-in-scikit-learn\n",
    "\n",
    "# dictionary of counts by actual party and estimated party. \n",
    "# first key is actual, second is estimated\n",
    "\n",
    "# Split the data into features and labels\n",
    "X = [tweet[0] for tweet in tweet_data]  \n",
    "y = [tweet[1] for tweet in tweet_data]  \n",
    "\n",
    "# Vectorize the tweets using CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X_vec = vectorizer.fit_transform(X)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_vec, y, test_size=0.3, \n",
    "                                                    random_state=42)\n",
    "\n",
    "# Train a Naive Bayes classifier\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "parties = ['Republican', 'Democratic']\n",
    "results = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "for p in parties:\n",
    "    for p1 in parties:\n",
    "        results[p][p1] = 0\n",
    "\n",
    "num_to_score = 10000\n",
    "random.shuffle(tweet_data)\n",
    "\n",
    "for idx, tp in enumerate(tweet_data):\n",
    "    tweet, actual_party = tp\n",
    "\n",
    "    # Now do the same thing as above, but we store the results rather\n",
    "    # than printing them. \n",
    "   \n",
    "    # Vectorize the tweet for classification\n",
    "    tweet_vec = vectorizer.transform([tweet])\n",
    "\n",
    "    # get the estimated party\n",
    "    estimated_party = clf.predict(tweet_vec)[0]\n",
    "\n",
    "    results[actual_party][estimated_party] += 1\n",
    "\n",
    "    if idx >= num_to_score:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>,\n",
       "            {'Republican': defaultdict(int,\n",
       "                         {'Republican': 3398, 'Democratic': 977}),\n",
       "             'Democratic': defaultdict(int,\n",
       "                         {'Republican': 890, 'Democratic': 4736})})"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflections\n",
    "\n",
    "_Write a little about what you see in the results_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results, it looks like the classifier did a decent job in classifying the tweets, but there’s still room for improvement. The Republican tweets were mostly classified correctly, with 3,398 Republican tweets correctly identified and 977 incorrectly labeled as Democratic. On the other hand, the Democratic tweets had more misclassifications, with 890 incorrectly labeled as Republican, but 4,736 correctly identified. This shows that while the classifier performs well with Republican tweets, it struggles more with Democratic ones. It may help to fine-tune the model, balance the dataset, or explore more sophisticated text processing techniques to improve accuracy.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook /Users/parisakamizi/ADS-509 Text Mining/Political Naive Bayes.ipynb to html\n",
      "[NbConvertApp] Writing 346295 bytes to /Users/parisakamizi/ADS-509 Text Mining/Political Naive Bayes.html\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to html \"/Users/parisakamizi/ADS-509 Text Mining/Political Naive Bayes.ipynb\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
